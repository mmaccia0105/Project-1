---
title: "Processing and Manipulating Data"
author: "Max Campbell and Mike Maccia"
format: html
editor: visual
---

```{r}
#|message=FALSE
#|warning=FALSE

# Load in necessary packages here!
suppressPackageStartupMessages(library(tidyverse))
```

## Data Processing

### First Steps: Importing Data

Our overall goal in this project is to process and manipulate data using common techniques and R packages. In this instance, we are using data from the 2010 census as an example! The first step in any data science project is to read in the data from wherever we are importing it from. In this case, we have some comma separated value (csv) files. Luckily, R makes this pretty easy with the `read.csv()` function!

```{r}
#|message=FALSE

# Read in the first dataset using read.csv
census_1 <- read.csv(file = "https://www4.stat.ncsu.edu/~online/datasets/EDU01a.csv", header = TRUE)

# Convert to a tibble for ease of use
census_1 <- as_tibble(census_1)
```

### 1: Selecting Columns

Now that we have some data to work with, we can use the `dplyr` package to select the columns we care about and take a look to make sure that everything got read in properly.

```{r}
# Select Area_name, STCOU, and any column ending with a D
census_1_small <- census_1 |>
  select(Area_name, STCOU, ends_with("D"))

#View our selection!
head(census_1_small, n = 5)
```

Looks good!

### 2: Pivoting Data

There is some information stored in the column names that we care about in our data, and it isn't exactly easy to analyze that data when it's stored in the name. As such, we are going to pivot this data into a longer dataframe.

```{r}
#Pivot longer so that we can access data stored in some column names
census_1_long <- census_1_small |>
  pivot_longer(cols = 3:12,
               names_to = "survey_ID",
               values_to = "enrollment")

#View results!
head(census_1_long, n = 5)
```

Notice how there is only one measurement per row now!

### 3: Parsing Strings

Let's take a closer look at the `survey_ID` column now. According to the data information sheet provided, the first three letters and first four numbers represent the type of survey that was completed. For example, row one has the value `EDU0101` which represents school survey_ID. The next two digits represent the year the survey was taken. Using the first row as an example again, we see that those two digits are `87`, meaning that the survey was taken in 1987. We can parse this column to get some meaningful data year-over-year.

```{r}
#Parse the survey_ID column:
#survey_ID - first three letters plus first four digits (ex. EDU0101)
#year - next two digits (ex. 87 becomes 1987)
census_1_long_updated <- census_1_long |>
  mutate(year = substr(survey_ID, 8, 9),
         survey_ID = substr(survey_ID, 1, 7)) |>
  #Add 1900 (or 2000) to the year column to get the YYYY instead of YY
  mutate(year = case_when(
    as.numeric(year) >= 87 ~ 1900 + as.numeric(year),
    .default = 2000 + as.numeric(year)
    )
  )

#View results!
head(census_1_long_updated, n = 5)
```

Now we have two columns to explain the survey information in a more intuitive manner.

### 4: Splitting the Dataset

Note that when we first imported the dataset, we saw that there was data for the country as a whole, as well as individual states as well as the counties within the states. If we wished to analyze the data, it may make more sense to be able to easily subset it by county and non-county data so we can get better insights. As such, let's split the data into a county set and a non-county set. We can use `dplyr`'s `slice()` function to accomplish this, using an expression provided that returns the indices of all rows that match a given condition!

```{r}
#Filter tibble into two separate tibbles using county as the identifier
county_1 <- census_1_long_updated |>
  slice(grep(pattern = ", \\w\\w", Area_name))

#We can invert using the 'invert' argument in grep() as well
non_county_1 <- census_1_long_updated |>
  slice(grep(pattern = ", \\w\\w", Area_name, invert = TRUE))

#Add the custom classes to the tibbles for future use
class(county_1) <- c("county", class(county_1))
class(non_county_1) <- c("state", class(non_county_1))

head(county_1, n = 10)
head(non_county_1, n = 10)
```

### 5: Splitting Strings in the County Data

We may want to subset by state in our analysis, so let's subset the county data to separate the county's name and state by using the comma as a delimiter.

```{r}
#Split the county name and state using , as a delimiter
county_1_split <- county_1 |>
  # separate() indicates that it is superseded per help file
  # Using separate_wider_delim() in favor of separate()
  separate_wider_delim(Area_name, ",", names = c("county", "state")) |>
  # Trim the whitespace on the state column
  mutate(state = trimws(state))

#View results
head(county_1_split, n = 5)
```

### 6: Designating Regions in Non-County Data

Similar to what we did in the county data above, we can describe the non-county data by larger groups. For the county data, this was the state each county was located in. For the non-county data, it will be the the divisions described by the [Census Bureau's designation](https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States). In other words, there are nine defined divisions that the Census Bureau that could be useful for our analysis. In order to indicate divisions we will have to use the `case_when` function in conjunction with the `%in%` operator to check and see if each entry is contained within a specific list of states belonging to a division.

```{r}
#Create string vectors of the states for clarity in the mutate call
ne <- c("CONNECTICUT", "MAINE", "MASSACHUSETTS", 
        "NEW HAMPSHIRE", "RHODE ISLAND", "VERMONT")
ma <- c("NEW JERSEY", "NEW YORK", "PENNSYLVANIA")
enc <- c("ILLINOIS", "INDIANA", "MICHIGAN", "OHIO", "WISCONSIN")
wnc <- c("IOWA", "KANSAS", "MINNESOTA", 
         "MISSOURI", "NEBRASKA", "NORTH DAKOTA", 
         "SOUTH DAKOTA")
sa <- c("DELAWARE", "DISTRICT OF COLUMBIA", "FLORIDA",
        "GEORGIA", "MARYLAND", "NORTH CAROLINA",
        "SOUTH CAROLINA", "VIRGINIA", "WEST VIRGINIA")
esc <- c("ALABAMA", "KENTUCKY", "MISSISSIPPI", "TENNESSEE")
wsc <- c("ARKANSAS", "LOUISIANA", "OKLAHOMA", "TEXAS")
mtn <- c("ARIZONA", "COLORADO", "IDAHO", 
         "MONTANA", "NEVADA", "NEW MEXICO", 
         "UTAH", "WYOMING")
pac <- c("ALASKA", "CALIFORNIA", "HAWAII", "OREGON", "WASHINGTON")

# Add the division variable to indicate Census-designated divisions
non_county_1_div <- non_county_1 |>
  mutate(division = case_when(
    Area_name %in% ne ~ "New England",
    Area_name %in% ma ~ "Mid-Atlantic",
    Area_name %in% enc ~ "East North Central",
    Area_name %in% wnc ~ "West North Central",
    Area_name %in% sa ~ "South Atlantic",
    Area_name %in% esc ~ "East South Central",
    Area_name %in% wsc ~ "West South Central",
    Area_name %in% mtn ~ "Mountain",
    Area_name %in% pac ~ "Pacific",
    .default = "ERROR"
  ))

#View results from the end 
#since the US data is first and will return ERROR
tail(non_county_1_div, n = 5)
```

With that, we've done everything that we'll need to do for our analysis! However, there is one key step that we need to do to make this process easily reproducible.

## Generalizing into Function Calls

This census data has two key characteristics that are relevant right now: the data that is imported follows the same general format, and we want to process the data in several different ways. That is, we want to easily be able to do the same processes without having to do a bunch of copying-and-pasting work. Luckily, we can write our own functions to take care of that!

There will have to be some generalizations made. For example, what if we want to observe data for a different metric? The census takes record of many different aspects of American demographics, and we may be interested in analyzing a different survey one day. Also, the latest observations that we have in the data we read in previously are from 1996, which is almost 30 years ago! We will want to read in some more recent records as well to get a bigger picture on survey_ID data.

So, we will write functions to generalize the processes performed above, and we will write a function to merge datasets in order to congregate our newer and older data in one (well, two, because we still want to split the data by county status) data frame(s)!

```{r}
#---STATE STRINGS---
#We will need these again for one of the helper functions
ne <- c("CONNECTICUT", "MAINE", "MASSACHUSETTS", 
        "NEW HAMPSHIRE", "RHODE ISLAND", "VERMONT")
ma <- c("NEW JERSEY", "NEW YORK", "PENNSYLVANIA")
enc <- c("ILLINOIS", "INDIANA", "MICHIGAN", "OHIO", "WISCONSIN")
wnc <- c("IOWA", "KANSAS", "MINNESOTA", 
         "MISSOURI", "NEBRASKA", "NORTH DAKOTA", 
         "SOUTH DAKOTA")
sa <- c("DELAWARE", "DISTRICT OF COLUMBIA", "FLORIDA",
        "GEORGIA", "MARYLAND", "NORTH CAROLINA",
        "SOUTH CAROLINA", "VIRGINIA", "WEST VIRGINIA")
esc <- c("ALABAMA", "KENTUCKY", "MISSISSIPPI", "TENNESSEE")
wsc <- c("ARKANSAS", "LOUISIANA", "OKLAHOMA", "TEXAS")
mtn <- c("ARIZONA", "COLORADO", "IDAHO", 
         "MONTANA", "NEVADA", "NEW MEXICO", 
         "UTAH", "WYOMING")
pac <- c("ALASKA", "CALIFORNIA", "HAWAII", "OREGON", "WASHINGTON")

#---HELPER FUNCTIONS---
#This is steps 1 and 2, selecting columns and pivoting data
select_and_pivot <- function(tib, name) {
  #Step 1: Select Columns
  result <- tib |>
    select(Area_name, STCOU, ends_with("D")) |>
    pivot_longer(cols = 3:12,
                 names_to = "survey_ID",
                 values_to = name)
  
  return(result)
}

#This is step 3, where we parse the old column names in survey_ID 
# as well as the survey_stat_name column
parse_survey_ID <- function(tib) {
  result <- tib |>
  mutate(year = substr(survey_ID, 8, 9),
         survey_ID = substr(survey_ID, 1, 7)) |>
  #Add 1900 (or 2000) to the year column to get the YYYY instead of YY
  mutate(year = case_when(
    as.numeric(year) >= 87 ~ 1900 + as.numeric(year),
    .default = 2000 + as.numeric(year)
    )
  )
    
  return(result)
}

#This is steps 4-6, where we split data and did some more parsing of strings
split_by_county_status <- function(tib) {
  #Take the county data and add state column
  result_c <- tib |>
    slice(grep(pattern = ", \\w\\w", Area_name)) |>
    # separate() indicates that it is superseded per help file
    # Using separate_wider_delim() in favor of separate()
    separate_wider_delim(Area_name, ",", names = c("county", "state")) |>
    # Trim the whitespace on the state column
    mutate(state = trimws(state))
  
  #Add custom county class
  class(result_c) <- c("county", class(result_c))
  
  #Take the non-county data and add division column
  result_nc <- tib |>
    slice(grep(pattern = ", \\w\\w", Area_name, invert = TRUE)) |>
    mutate(division = case_when(
    Area_name %in% ne ~ "New England",
    Area_name %in% ma ~ "Mid-Atlantic",
    Area_name %in% enc ~ "East North Central",
    Area_name %in% wnc ~ "West North Central",
    Area_name %in% sa ~ "South Atlantic",
    Area_name %in% esc ~ "East South Central",
    Area_name %in% wsc ~ "West South Central",
    Area_name %in% mtn ~ "Mountain",
    Area_name %in% pac ~ "Pacific",
    .default = "ERROR"
  ))
  
  #Add custom state class
  class(result_nc) <- c("state", class(result_nc))
  
  #Return a list of two tibbles
  return(list(result_c, result_nc))
}

#---MAIN FUNCTION CALL---
#This just calls all the helper functions in order 
#and returns the final product
process_census_data <- function(url, survey_stat_name = "enrollment") {
  #Quick sanity check to make sure we have a character string
  if (!is.character(url)) {
    stop("url argument must be a character string!")
  }
  
  #Run through the helper functions and return the results
  result <- as_tibble(read.csv(file = url, header = TRUE)) |>
   select_and_pivot(name = survey_stat_name) |>
   parse_survey_ID() |>
   split_by_county_status()
  
  #Retuns a list of 2 tibbles
  return(result)
}

#---DATA COMBINING FUNCTION---
#This will take two lists of two tibbles each and
# combine them into one list of two tibbles

combine_census_data <- function(x, y)  {
  #Sanity checks: make sure each list is a list of size 2
  if(!is_tibble(x[[1]]) || !is_tibble(x[[2]])) {
    stop("First list argument does not exclusively contain tibbles.")
  }
  
  if(!is_tibble(y[[1]]) || !is_tibble(y[[2]])) {
    stop("Second list argument does not exclusively contain tibbles.")
  }
  
  if(length(x) != length(y) || length(x) != 2) {
    stop("Lists must be of length 2. Current length: ", length(x))
  }
  
  #Combine the first element of each list (county data) together
  result_c <- bind_rows(x[[1]], y[[1]])
  #Combine the second element of each list (non-county data) together
  result_nc <- bind_rows(x[[2]], y[[2]])
  
  #Return the result datasets!
  return(list(result_c, result_nc))
}
```

Now that we've written our functions, all that's left to do is call them. If everything works properly, we should see a list of two different tibbles: one with county data from 1987-2006, and another with non-county data from 1987-2006. These datasets should be formatted exactly like the example data we used above!

```{r}
#Call the functions we just made!
list1 <- process_census_data(url = "https://www4.stat.ncsu.edu/~online/datasets/EDU01a.csv", survey_stat_name = "enrollment_count")
list2 <- process_census_data(url = "https://www4.stat.ncsu.edu/~online/datasets/EDU01b.csv", survey_stat_name = "enrollment_count")

#Combine the two lists together
census <- combine_census_data(list1, list2)
```

Let's do a quick sort and take a look at some of the data to see if everything looks okay.

```{r}
#View results
census[[1]] |>
  arrange(county, year) |>
  head(n = 20)
```

Looks good on the county side. Let's check the non-counties as well.

```{r}
#View results, from the end this time for the sake of it.
census[[2]] |>
  arrange(division, Area_name, year) |>
  tail(n = 20)
```

Looks good here, too. Now that we have a way to easily process the data we are looking for, it's time to start thinking about how we are going to summarize and analyze it.
